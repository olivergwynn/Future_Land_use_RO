{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5b15b1-ddd8-4b4f-b64a-11edc357ae86",
   "metadata": {},
   "source": [
    "# Future Land Use RO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81780ee-1d31-4634-a8d4-e79ed9af240c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b6a72-f5d2-415e-8334-3b9ab1d7a5dd",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of preparing a Research Object Crate (RO-Crate) for the Future Land Use dataset. The dataset contains spatial and attribute information. The spatial data will be stored in a Shapefile containing just the geographies and a unique identifier, and the attribute data will be stored in a CSV file containing the unique identifier and the remaining fields. The CSV file will be structured according to a Frictionless table schema to ensure consistency and quality. Additionally, the notebook will export the resulting data as a GeoPackage for further use. The entire process includes data extraction, transformation, validation, and packaging into a research object crate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f990fc-5615-4d62-997a-9f05e1fa8c1b",
   "metadata": {},
   "source": [
    "## Process Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492c047-7af3-4820-a5ae-0c4e03117ac3",
   "metadata": {},
   "source": [
    "The process carried out by this workflow can be described as follows:\n",
    "  - Set Up: Import necessary packages and define parameters for file paths and output locations.\n",
    "  - Extract and Split Data:\n",
    "    -  Extract the ZIP file containing the Future Land Use dataset.\n",
    "    -  Read the CSV file and split it into two separate files: one for geographies and one for attributes. \n",
    "  - Prepare RO-Crate:\n",
    "    -  Create a Frictionless table schema for the attributes CSV file.\n",
    "    -  Generate RO-Crate metadata and save it as a JSON file.\n",
    "    -  Package the CSV files, schema, and metadata into a ZIP file to create the RO-Crate.   \n",
    "  -  Export GeoPackage:\n",
    "    -  Read the Shapefile and convert its coordinate reference system to Ohio State Plane South (EPSG:3735).\n",
    "    -  Join the attributes CSV file with the Shapefile and export the resulting data as a GeoPackage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af112a-e926-4560-a8be-b6c991f85508",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd4d88-ab48-41c0-9d2b-1a2370a3bc80",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d0c735-dd75-4faf-b72c-a4b583580925",
   "metadata": {},
   "outputs": [],
   "source": [
    "## .scehma.yaml AND .resource.yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "import frictionless\n",
    "import shutil\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append(os.path.normpath(\"../morpc-common\"))\n",
    "import morpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5b6d-7960-41f6-8bf3-595558ef5952",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1da9c2-1ae6-475b-9015-96c9d829dde9",
   "metadata": {},
   "source": [
    "#### Static Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2154c339-bb80-404a-a58f-e9035fbb150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories and file paths\n",
    "OUTPUT_DIR = os.path.normpath(\"./output_data\")\n",
    "INPUT_DIR = os.path.normpath(\"./input_data\")\n",
    "EXTRACTED_DIR = 'shapefile_extracted'\n",
    "\n",
    "# Define INPUT file name and path for zip, csv, and xlsx\n",
    "ZIP_NAME = 'Future_Land_use__MTP2024_parcels_Symbology.zip'\n",
    "ZIP_INPUT_PATH = os.path.join(INPUT_DIR, ZIP_NAME)\n",
    "FUTURE_LAND_USE_INPUT_NAME = \"Future_Land_use__MTP2024_parcels_Symbology.csv\"\n",
    "FUTURE_LAND_USE_INPUT_PATH = os.path.join(INPUT_DIR, FUTURE_LAND_USE_INPUT_NAME)\n",
    "TYPE_DESCIP_NAME = \"LU_Standardized LandUse Type Descriptions.xlsx\"\n",
    "TYPE_DESCIP_PATH = os.path.join(INPUT_DIR, TYPE_DESCIP_NAME)\n",
    "\n",
    "# Define OUTPUT shapefile name and path for data, schema, and resource files\n",
    "OUTPUT_SHAPEFILE_DIR = os.path.join(OUTPUT_DIR, 'filtered_shapefile')\n",
    "OUTPUT_SHAPEFILE_PATH = os.path.join(OUTPUT_SHAPEFILE_DIR, 'filtered_data.shp')\n",
    "ZIP_NAME = 'Future_Land_use__MTP2024_filtered.zip'\n",
    "ZIP_OUTPUT_PATH = os.path.join(OUTPUT_DIR, ZIP_NAME)\n",
    "SHAPEFILE_RESOURCE_FILE_PATH = os.path.join(OUTPUT_DIR, 'Future_Land_use__MTP2024_filtered.resource.yaml')\n",
    "\n",
    "# Define SPLIT DEFINITIONS file name and path for data, schema, and resource files\n",
    "TYPE_DESCIP_LUT_NAME = 'Land_Use_Types_descriptions.csv'\n",
    "TYPE_DESCIP_LUT_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_LUT_NAME)\n",
    "TYPE_DESCIP_MIXU_NAME = 'Mixed_Use_descriptions.csv'\n",
    "TYPE_DESCIP_MIXU_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_MIXU_NAME)\n",
    "TYPE_DESCIP_LUT_RESOURCE_NAME = 'Land_Use_Types_descriptions.resource.yaml'\n",
    "TYPE_DESCIP_LUT_RESOURCE_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_LUT_RESOURCE_NAME)\n",
    "TYPE_DESCIP_MIXU_RESOURCE_NAME = 'Mixed_Use_descriptions.resource.yaml'\n",
    "TYPE_DESCIP_MIXU_RESOURCE_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_MIXU_RESOURCE_NAME)\n",
    "TYPE_DESCIP_MIXU_SCHEMA_NAME = 'Mixed_Use_descriptions.schema.yaml'\n",
    "TYPE_DESCIP_MIXU_SCHEMA_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_MIXU_SCHEMA_NAME)\n",
    "TYPE_DESCIP_LUT_SCHEMA_NAME = 'Land_Use_Types_descriptions.schema.yaml'\n",
    "TYPE_DESCIP_LUT_SCHEMA_PATH = os.path.join(OUTPUT_DIR, TYPE_DESCIP_LUT_SCHEMA_NAME)\n",
    "\n",
    "# Define non-geographic attributes file name and path for data, schema, and resource files\n",
    "FUTURE_LAND_USE_ATTRIB_OUTPUT_NAME = \"Future_Land_use_attributes.csv\"\n",
    "FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH = os.path.join(OUTPUT_DIR, FUTURE_LAND_USE_ATTRIB_OUTPUT_NAME)\n",
    "ATTRIBUTES_SCHEMA_NAME = 'Future_Land_use_attributes.schema.yaml'\n",
    "ATTRIBUTES_SCHEMA_PATH = os.path.join(OUTPUT_DIR, ATTRIBUTES_SCHEMA_NAME)\n",
    "ATTRIBUTES_RESOURCE_NAME = 'Future_Land_use_attributes.resource.yaml'\n",
    "ATTRIBUTES_RESOURCE_PATH = os.path.join(OUTPUT_DIR, ATTRIBUTES_RESOURCE_NAME)\n",
    "\n",
    "# Define file name and path for zipped RO-Crate and metadata\n",
    "RO_CRATE_METADATA_NAME = 'ro-crate-metadata.json'\n",
    "RO_CRATE_METADATA_PATH = os.path.join(OUTPUT_DIR, RO_CRATE_METADATA_NAME)\n",
    "RO_CRATE_NAME = 'future-land-use-crated.zip'\n",
    "RO_CRATE_PATH = os.path.join(OUTPUT_DIR, RO_CRATE_NAME)\n",
    "\n",
    "# Define file name and path for GeoPackage\n",
    "OUTPUT_GEOPACKAGE_NAME = 'Future_Land_use.gpkg'\n",
    "OUTPUT_GEOPACKAGE_PATH = os.path.join(OUTPUT_DIR, OUTPUT_GEOPACKAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679d8c8-2a0f-4ab6-8e8f-cacb576001c6",
   "metadata": {},
   "source": [
    "### Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb96e6ab-13c1-46c9-8aca-0c764cd8a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped Future Land Use shapefile taken from: https://opendata.arcgis.com/api/v3/datasets/22ac0071b4234a41a0414e0c9121e23f_20/downloads/data?format=shp&spatialRefId=3735&where=1%3D1\n"
     ]
    }
   ],
   "source": [
    "print(\"Zipped Future Land Use shapefile taken from: {}\".format(\"https://opendata.arcgis.com/api/v3/datasets/22ac0071b4234a41a0414e0c9121e23f_20/downloads/data?format=shp&spatialRefId=3735&where=1%3D1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174ac7d-b778-4d0f-92d2-ee26328ab416",
   "metadata": {},
   "source": [
    "### Define Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530351e2-e5b2-420f-854d-b97236b3353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RO-Crate saved as: output_data\\future-land-use-crated.zip\n",
      "Exported GeoPackage saved as: output_data\\Future_Land_use.gpkg\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered RO-Crate saved as: {}\".format(RO_CRATE_PATH))\n",
    "print(\"Exported GeoPackage saved as: {}\".format(OUTPUT_GEOPACKAGE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202f0d7-89b5-4a74-8f72-4c9032770350",
   "metadata": {},
   "source": [
    "## Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9623c-7c72-499a-9e2a-db90c956c33b",
   "metadata": {},
   "source": [
    "### Download and extract zip file, load shapefile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8c4e1f-b3e0-4ebb-a6fc-a8bcc3382561",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_url = \"https://opendata.arcgis.com/api/v3/datasets/22ac0071b4234a41a0414e0c9121e23f_20/downloads/data?format=shp&spatialRefId=3735&where=1%3D1\" # Change path as nessecary\n",
    "\n",
    "# Step 1: Download the shapefile zip\n",
    "response = requests.get(shapefile_url)\n",
    "with open(ZIP_INPUT_PATH, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Step 2: Extract the downloaded zip file\n",
    "with zipfile.ZipFile(ZIP_INPUT_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(EXTRACTED_DIR)\n",
    "\n",
    "# Step 3: Load the data\n",
    "gdf = gpd.read_file(EXTRACTED_DIR)\n",
    "\n",
    "# Step 4: Export the filtered GeoDataFrame to a Shapefile\n",
    "os.makedirs(OUTPUT_SHAPEFILE_DIR, exist_ok=True)\n",
    "required_fields = ['OBJECTID', 'geometry']\n",
    "filtered_gdf = gdf[required_fields]\n",
    "filtered_gdf.to_file(OUTPUT_SHAPEFILE_PATH, driver='ESRI Shapefile')\n",
    "\n",
    "# Step 5: Create a new zip file containing the filtered shapefile\n",
    "with zipfile.ZipFile(ZIP_OUTPUT_PATH, 'w') as zipf:\n",
    "    for root, _, files in os.walk(OUTPUT_SHAPEFILE_DIR):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, OUTPUT_SHAPEFILE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ecd8f-d157-4758-a448-55f48d4dcbe4",
   "metadata": {},
   "source": [
    "#### Save and format attribute csv from shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0818f0a3-43a3-4d53-bc7a-3f524a654d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered shapefile created, zipped, and temporary files removed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the required fields, excluding 'Shape__Area' and 'Shape__Length'\n",
    "required_fields = [col for col in gdf.columns if col not in ['Shape__Are', 'Shape__Len', 'geometry']]\n",
    "attributes_df = gdf[required_fields]\n",
    "\n",
    "# Step 2: Save the attributes to a CSV file\n",
    "attributes_df = attributes_df[required_fields]\n",
    "\n",
    "# Step 3: Rename the column 'last_edite' to 'last_edited_date'\n",
    "if 'last_edite' in attributes_df.columns:\n",
    "    attributes_df.rename(columns={'last_edite': 'last_edited_date'}, inplace=True)\n",
    "attributes_df.to_csv(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"Filtered shapefile created, zipped, and temporary files removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecc742-5661-4e62-b069-f8db2aaf2afe",
   "metadata": {},
   "source": [
    "#### Split the Excel file and save land use and mixed use CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7002ee-b4c5-4ec6-9d04-d6ff9dcb4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. CSV files saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load each sheet into a DataFrame\n",
    "land_use_df = pd.read_excel(TYPE_DESCIP_PATH, sheet_name='Land Use Types')\n",
    "mixed_use_df = pd.read_excel(TYPE_DESCIP_PATH, sheet_name='Mixed Use')\n",
    "\n",
    "# Step 2: Clean the DataFrames\n",
    "mixed_use_df.dropna(inplace=True)\n",
    "mixed_use_df = mixed_use_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "mixed_use_df = mixed_use_df[(mixed_use_df != '').all(axis=1)]\n",
    "\n",
    "land_use_df.dropna(inplace=True)\n",
    "land_use_df = land_use_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "land_use_df = land_use_df[(land_use_df != '').all(axis=1)]\n",
    "\n",
    "# Step 3: Save the cleaned DataFrames to CSV\n",
    "land_use_df.to_csv(TYPE_DESCIP_LUT_PATH, index=False)\n",
    "mixed_use_df.to_csv(TYPE_DESCIP_MIXU_PATH, index=False)\n",
    "\n",
    "print(\"Data processing complete. CSV files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181046b-e05a-4e95-b104-b1ecd5822d51",
   "metadata": {},
   "source": [
    "### Create and validate resource files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90ab16-0bfc-42b9-89d4-9bd6cf75f1cc",
   "metadata": {},
   "source": [
    "#### Non-Geographic attribute csv resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b418edad-107e-490d-bbfc-f8bb467ecd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resource file to output_data\\Future_Land_use_attributes.resource.yaml\n",
      "Validating resource on disk (including data and schema). This may take some time.\n",
      "Resource is valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH) and os.path.getsize(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH) > 0:\n",
    "\n",
    "    df_temp = pd.read_csv(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH, low_memory = False)\n",
    "\n",
    "    # Resource creation for WIDE ANNUAL\n",
    "    if not df_temp.empty:\n",
    "        acsResource = {\n",
    "            \"name\": \"future_land_use__attribute\",\n",
    "            \"title\": \"future_land_use__attribute\",\n",
    "            \"description\": \"future_land_use__attribute\",\n",
    "            \"path\": FUTURE_LAND_USE_ATTRIB_OUTPUT_NAME,\n",
    "            \"format\": \"csv\",\n",
    "            \"mediatype\": \"text/csv\",\n",
    "            \"encoding\": \"utf-8\",\n",
    "            \"bytes\": os.path.getsize(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH),\n",
    "            \"hash\": morpc.md5(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH),\n",
    "            \"schema\": ATTRIBUTES_SCHEMA_NAME,\n",
    "            \"profile\":'tabular-data-resource'\n",
    "        }\n",
    "    \n",
    "        # Create the resource object\n",
    "        resource = frictionless.Resource(acsResource)\n",
    "\n",
    "        print(\"Writing resource file to {}\".format(ATTRIBUTES_RESOURCE_PATH))\n",
    "        cwd = os.getcwd()\n",
    "        os.chdir(os.path.dirname(ATTRIBUTES_RESOURCE_PATH))\n",
    "        dummy = resource.to_yaml(os.path.basename(ATTRIBUTES_RESOURCE_PATH))\n",
    "        os.chdir(cwd)\n",
    "    \n",
    "        print(\"Validating resource on disk (including data and schema). This may take some time.\")\n",
    "        resourceOnDisk = frictionless.Resource(ATTRIBUTES_RESOURCE_PATH)\n",
    "        results = resourceOnDisk.validate()\n",
    "        if(results.valid):\n",
    "            print(\"Resource is valid\\n\")\n",
    "        else:\n",
    "            print(\"ERROR: Resource is NOT valid. Errors follow.\\n\")\n",
    "            print(results)\n",
    "            raise RuntimeError\n",
    "\n",
    "else:\n",
    "    print(f\"{FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH} does not exist or is empty\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441f54f-576c-4aee-87e4-611380c02aa7",
   "metadata": {},
   "source": [
    "#### Land Use Types descriptions resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0542f357-138f-4c5c-8165-388078115a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resource file to output_data\\Land_Use_Types_descriptions.resource.yaml\n",
      "Validating resource on disk (including data and schema). This may take some time.\n",
      "Resource is valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TYPE_DESCIP_LUT_PATH) and os.path.getsize(TYPE_DESCIP_LUT_PATH) > 0:\n",
    "\n",
    "    df_temp = pd.read_csv(TYPE_DESCIP_LUT_PATH)\n",
    "    \n",
    "    # Resource creation for WIDE ANNUAL\n",
    "    if not df_temp.empty:\n",
    "        acsResource = {\n",
    "            \"name\": \"land_use_types_descriptions\",\n",
    "            \"title\": \"land_use_types_descriptions\",\n",
    "            \"description\": \"land_use_types_descriptions\",\n",
    "            \"path\": TYPE_DESCIP_LUT_NAME,\n",
    "            \"format\": \"csv\",\n",
    "            \"mediatype\": \"text/csv\",\n",
    "            \"encoding\": \"utf-8\",\n",
    "            \"bytes\": os.path.getsize(TYPE_DESCIP_LUT_PATH),\n",
    "            \"hash\": morpc.md5(TYPE_DESCIP_LUT_PATH),\n",
    "            \"schema\": TYPE_DESCIP_LUT_SCHEMA_NAME,\n",
    "            \"profile\":'tabular-data-resource'\n",
    "        }\n",
    "    \n",
    "        # Create the resource object\n",
    "        resource = frictionless.Resource(acsResource)\n",
    "\n",
    "        print(\"Writing resource file to {}\".format(TYPE_DESCIP_LUT_RESOURCE_PATH))\n",
    "        cwd = os.getcwd()\n",
    "        os.chdir(os.path.dirname(TYPE_DESCIP_LUT_RESOURCE_PATH))\n",
    "        dummy = resource.to_yaml(os.path.basename(TYPE_DESCIP_LUT_RESOURCE_PATH))\n",
    "        os.chdir(cwd)\n",
    "    \n",
    "        print(\"Validating resource on disk (including data and schema). This may take some time.\")\n",
    "        resourceOnDisk = frictionless.Resource(TYPE_DESCIP_LUT_RESOURCE_PATH)\n",
    "        results = resourceOnDisk.validate()\n",
    "        if(results.valid):\n",
    "            print(\"Resource is valid\\n\")\n",
    "        else:\n",
    "            print(\"ERROR: Resource is NOT valid. Errors follow.\\n\")\n",
    "            print(results)\n",
    "            raise RuntimeError\n",
    "\n",
    "else:\n",
    "    print(f\"{TYPE_DESCIP_LUT_RESOURCE_PATH} does not exist or is empty\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483cf53-805d-417c-b286-a4f283f1c001",
   "metadata": {},
   "source": [
    "#### Mixed use desciptions resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8760e786-539d-4a43-bc10-49a8035ecd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resource file to output_data\\Mixed_Use_descriptions.resource.yaml\n",
      "Validating resource on disk (including data and schema). This may take some time.\n",
      "Resource is valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TYPE_DESCIP_MIXU_PATH) and os.path.getsize(TYPE_DESCIP_MIXU_PATH) > 0:\n",
    "\n",
    "    df_temp = pd.read_csv(TYPE_DESCIP_MIXU_PATH)\n",
    "\n",
    "    # Resource creation for WIDE ANNUAL\n",
    "    if not df_temp.empty:\n",
    "        acsResource = {\n",
    "            \"name\": \"mixed_use_descriptions\",\n",
    "            \"title\": \"mixed_use_descriptions\",\n",
    "            \"description\": \"mixed_use_descriptions\",\n",
    "            \"path\": TYPE_DESCIP_MIXU_NAME,\n",
    "            \"format\": \"csv\",\n",
    "            \"mediatype\": \"text/csv\",\n",
    "            \"encoding\": \"utf-8\",\n",
    "            \"bytes\": os.path.getsize(TYPE_DESCIP_MIXU_PATH),\n",
    "            \"hash\": morpc.md5(TYPE_DESCIP_MIXU_PATH),\n",
    "            \"schema\": TYPE_DESCIP_MIXU_SCHEMA_NAME,\n",
    "            \"profile\":'tabular-data-resource'\n",
    "        }\n",
    "    \n",
    "        # Create the resource object\n",
    "        resource = frictionless.Resource(acsResource)\n",
    "\n",
    "        print(\"Writing resource file to {}\".format(TYPE_DESCIP_MIXU_RESOURCE_PATH))\n",
    "        cwd = os.getcwd()\n",
    "        os.chdir(os.path.dirname(TYPE_DESCIP_MIXU_RESOURCE_PATH))\n",
    "        dummy = resource.to_yaml(os.path.basename(TYPE_DESCIP_MIXU_RESOURCE_PATH))\n",
    "        os.chdir(cwd)\n",
    "    \n",
    "        print(\"Validating resource on disk (including data and schema). This may take some time.\")\n",
    "        resourceOnDisk = frictionless.Resource(TYPE_DESCIP_MIXU_RESOURCE_PATH)\n",
    "        results = resourceOnDisk.validate()\n",
    "        if(results.valid):\n",
    "            print(\"Resource is valid\\n\")\n",
    "        else:\n",
    "            print(\"ERROR: Resource is NOT valid. Errors follow.\\n\")\n",
    "            print(results)\n",
    "            raise RuntimeError\n",
    "\n",
    "else:\n",
    "    print(f\"{TYPE_DESCIP_MIXU_RESOURCE_PATH} does not exist or is empty\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a04f6-9fc4-4240-9927-974fcea6c051",
   "metadata": {},
   "source": [
    "#### Filtered shapefile resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95fa9172-fe2d-4a2e-b02f-f82b2fb0a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resource file to output_data\\Future_Land_use__MTP2024_filtered.resource.yaml\n",
      "Validating resource on disk (including data and schema). This may take some time.\n",
      "Resource is valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "schema = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"OBJECTID\", \"type\": \"integer\", \"constraints\": {\"required\": True}},\n",
    "        {\"name\": \"geometry\", \"type\": \"string\", \"constraints\": {\"required\": True}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "acsResource = {\n",
    "    \"name\": \"future_land_use__mtp2024_filtered\",\n",
    "    \"title\": \"Future Land Use MTP 2024 Filtered Parcels Symbology\",\n",
    "    \"description\": \"Filtered shapefile containing the OBJECTID and geometry fields.\",\n",
    "    \"path\": ZIP_NAME,\n",
    "    \"format\": \"zip\",\n",
    "    \"mediatype\": \"application/zip\",\n",
    "    \"encoding\": \"utf-8\",\n",
    "    \"bytes\": os.path.getsize(ZIP_OUTPUT_PATH),\n",
    "    \"hash\": morpc.md5(ZIP_OUTPUT_PATH),\n",
    "    \"schema\": schema,\n",
    "    \"profile\": 'data-resource'\n",
    "}\n",
    "\n",
    "# Create the resource object\n",
    "resource = frictionless.Resource(acsResource)\n",
    "\n",
    "print(\"Writing resource file to {}\".format(SHAPEFILE_RESOURCE_FILE_PATH))\n",
    "cwd = os.getcwd()\n",
    "os.chdir(os.path.dirname(SHAPEFILE_RESOURCE_FILE_PATH))\n",
    "resource.to_yaml(os.path.basename(SHAPEFILE_RESOURCE_FILE_PATH))\n",
    "os.chdir(cwd)\n",
    "\n",
    "print(\"Validating resource on disk (including data and schema). This may take some time.\")\n",
    "resourceOnDisk = frictionless.Resource(SHAPEFILE_RESOURCE_FILE_PATH)\n",
    "results = resourceOnDisk.validate()\n",
    "if results.valid:\n",
    "    print(\"Resource is valid\\n\")\n",
    "else:\n",
    "    print(\"ERROR: Resource is NOT valid. Errors follow.\\n\")\n",
    "    print(results)\n",
    "    raise RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b36528-3ad8-4552-bcc3-3f7f127c34c2",
   "metadata": {},
   "source": [
    "### Step 4: Preparing RO Crate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8b5a6-6971-49be-8766-81ed4cf48c94",
   "metadata": {},
   "source": [
    "#### RO-Crate method definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa0e07a-03c4-4345-8ff3-3be77d06ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_write_ro_crate_metadata(directory, output_file, omit_names=None):\n",
    "    \"\"\"\n",
    "    Create the RO-Crate metadata for the given directory and write it to a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    if omit_names is None:\n",
    "        omit_names = []\n",
    "\n",
    "    def should_omit(name):\n",
    "        \"\"\"\n",
    "        Determine if a file or directory should be omitted based on the given omit names.\n",
    "        \"\"\"\n",
    "        return any(name.endswith(ext) for ext in ['.git', '.ipynb_checkpoints']) or name in omit_names\n",
    "\n",
    "    def list_directory_contents(directory):\n",
    "        \"\"\"\n",
    "        Recursively list the contents of the given directory, ignoring specified files and directories.\n",
    "        \"\"\"\n",
    "        contents = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Ignore directories ending with .git and .ipynb_checkpoints, and other specified names\n",
    "            dirs[:] = [d for d in dirs if not should_omit(d)]\n",
    "            for name in files:\n",
    "                if should_omit(name):\n",
    "                    continue\n",
    "                path = os.path.join(root, name)\n",
    "                contents.append({\n",
    "                    \"type\": \"File\",\n",
    "                    \"path\": os.path.relpath(path, directory),\n",
    "                    \"root\": root\n",
    "                })\n",
    "            for name in dirs:\n",
    "                if should_omit(name):\n",
    "                    continue\n",
    "                path = os.path.join(root, name)\n",
    "                contents.append({\n",
    "                    \"type\": \"Directory\",\n",
    "                    \"path\": os.path.relpath(path, directory),\n",
    "                    \"root\": root\n",
    "                })\n",
    "        return contents\n",
    "\n",
    "    def get_description_from_resource_file(resource_file):\n",
    "        \"\"\"\n",
    "        Get the description from the resource file if it exists.\n",
    "        \"\"\"\n",
    "        if os.path.exists(resource_file):\n",
    "            with open(resource_file, 'r') as file:\n",
    "                resource_data = yaml.safe_load(file)\n",
    "                return resource_data.get('description', '')\n",
    "        return ''\n",
    "\n",
    "    # Create the RO-Crate metadata structure\n",
    "    metadata = {\n",
    "        \"@context\": \"https://w3id.org/ro/crate/1.1/context\",\n",
    "        \"@graph\": []\n",
    "    }\n",
    "    \n",
    "    # Add the root dataset\n",
    "    metadata[\"@graph\"].append({\n",
    "        \"@id\": \"./\",\n",
    "        \"@type\": \"Dataset\",\n",
    "        \"name\": os.path.basename(directory),\n",
    "        \"dateCreated\": datetime.now().isoformat(),\n",
    "        \"hasPart\": []\n",
    "    })\n",
    "    \n",
    "    # List directory contents\n",
    "    contents = list_directory_contents(directory)\n",
    "    \n",
    "    # Add contextual entities for each file and directory\n",
    "    path_to_entity = {}\n",
    "    for item in contents:\n",
    "        entity = {\n",
    "            \"@id\": item[\"path\"],\n",
    "            \"@type\": item[\"type\"],\n",
    "            \"name\": os.path.basename(item[\"path\"])\n",
    "        }\n",
    "        # Check for corresponding resource file and get description\n",
    "        if item[\"type\"] == \"File\" and not item[\"path\"].endswith(\".resource.yaml\"):\n",
    "            base_name = os.path.splitext(item[\"path\"])[0]\n",
    "            resource_file = f\"{base_name}.resource.yaml\"\n",
    "            if os.path.exists(resource_file):\n",
    "                description = get_description_from_resource_file(resource_file)\n",
    "                entity[\"description\"] = description\n",
    "\n",
    "            # Calculate and add MD5 checksum\n",
    "            file_full_path = os.path.join(directory, item[\"path\"])\n",
    "            entity[\"contentChecksum\"] = [{\n",
    "                \"checksumAlgorithm\": \"MD5\",\n",
    "                \"checksumValue\": morpc.md5(file_full_path)\n",
    "            }]\n",
    "        \n",
    "        metadata[\"@graph\"].append(entity)\n",
    "        path_to_entity[item[\"path\"]] = entity\n",
    "\n",
    "    # Establish hasPart relationships and link data files with their resource files\n",
    "    for item in contents:\n",
    "        parent_path = os.path.relpath(item[\"root\"], directory)\n",
    "        if parent_path == \".\":\n",
    "            parent_path = \"./\"\n",
    "        \n",
    "        parent_entity = path_to_entity.get(parent_path, None)\n",
    "        if parent_entity:\n",
    "            if \"hasPart\" not in parent_entity:\n",
    "                parent_entity[\"hasPart\"] = []\n",
    "            parent_entity[\"hasPart\"].append({\"@id\": item[\"path\"]})\n",
    "\n",
    "        # Check for corresponding resource file and link it\n",
    "        if item[\"type\"] == \"File\" and not item[\"path\"].endswith(\".resource.yaml\"):\n",
    "            base_name = os.path.splitext(item[\"path\"])[0]\n",
    "            resource_file = f\"{base_name}.resource.yaml\"\n",
    "            if resource_file in path_to_entity:\n",
    "                item_entity = path_to_entity[item[\"path\"]]\n",
    "                item_entity[\"isDescribedBy\"] = {\"@id\": resource_file}\n",
    "    \n",
    "    # Write the metadata to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a30e6-a90d-4d92-85ba-a8e47a112765",
   "metadata": {},
   "source": [
    "#### Calling RO-Crate Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7430f3c4-a27a-4702-8471-b95f8e0910a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add other file names to omit\n",
    "omit_names = ['filtered_shapefile','shapefile_extracted', 'example_dir', 'example_file.txt']  # Example of additional names to omit\n",
    "\n",
    "# Calling RO-Crate method from MORPC Common\n",
    "# morpc.create_and_write_ro_crate_metadata(current_directory, \"ro-crate-metadata.json\")\n",
    "\n",
    "# Calling RO-Crate method locally\n",
    "create_and_write_ro_crate_metadata(current_directory, \"ro-crate-metadata.json\", omit_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05d9dd-845b-47a9-8b91-481ad464e400",
   "metadata": {},
   "source": [
    "### Step 5: Exporting standard GeoPackage from Shapefile geodataframe and CSV dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720b6e07-4e71-4427-81c1-d2b02589b484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoPackage has been saved to output_data\\Future_Land_use.gpkg\n"
     ]
    }
   ],
   "source": [
    "# Set to False to skip Geopackage creation\n",
    "create_geopackage = True \n",
    "\n",
    "# Export the resulting geodataframe as a GeoPackage if the parameter is true\n",
    "if create_geopackage:\n",
    "    # Read the CSV file as a dataframe\n",
    "    attributes_df = pd.read_csv(FUTURE_LAND_USE_ATTRIB_OUTPUT_PATH, low_memory=False)\n",
    "\n",
    "    # Read the Shapefile as a geodataframe\n",
    "    shapefile_gdf = gpd.read_file(OUTPUT_SHAPEFILE_PATH)\n",
    "\n",
    "    # Convert the Shapefile to Ohio State Plane South coordinate reference system\n",
    "    shapefile_gdf = shapefile_gdf.to_crs(epsg=3735)\n",
    "\n",
    "    # Ensure the 'OBJECTID' column is the index for join operation\n",
    "    attributes_df.set_index('OBJECTID', inplace=True)\n",
    "\n",
    "    # Rename overlapping columns in the attributes_df to avoid conflicts\n",
    "    overlap_cols = attributes_df.columns.intersection(shapefile_gdf.columns)\n",
    "    attributes_df.rename(columns={col: col + '_attr' for col in overlap_cols}, inplace=True)\n",
    "    \n",
    "    # Join the CSV dataframe to the Shapefile geodataframe using the unique identifier field\n",
    "    merged_gdf = shapefile_gdf.join(attributes_df, on='OBJECTID', how='inner')\n",
    "    \n",
    "    merged_gdf.to_file(OUTPUT_GEOPACKAGE_PATH, driver='GPKG')\n",
    "    print(f\"GeoPackage has been saved to {OUTPUT_GEOPACKAGE_PATH}\")\n",
    "else:\n",
    "    print(\"GeoPackage creation is skipped.\")\n",
    "\n",
    "# Delete the temporary directories and  contents\n",
    "if os.path.exists(EXTRACTED_DIR):\n",
    "    shutil.rmtree(EXTRACTED_DIR)\n",
    "\n",
    "if os.path.exists(OUTPUT_SHAPEFILE_DIR):\n",
    "    shutil.rmtree(OUTPUT_SHAPEFILE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
